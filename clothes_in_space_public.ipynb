{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import string\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # for viz purposes\n",
    "from sklearn.manifold import TSNE  # for viz purposes\n",
    "import redis  # to communicate with redis\n",
    "import gensim # to talk to gensim\n",
    "from sklearn.decomposition import PCA # run PCA\n",
    "from IPython.display import Image  # to display URL in noteboook for visual debug\n",
    "from IPython.core.display import display # to display URL in noteboook for visual debug\n",
    "from elasticsearch import Elasticsearch, helpers # remember to !pip install elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOGUE_FILE = './data/catalog.csv'  # put here your catalog file\n",
    "SESSION_FILE = './data/sessions.txt' # file with session data (pre-filtered for length and pre-formatted)\n",
    "TEXT_FILE = '/tmp/corpus.txt'  # texts from 1BN words dataset\n",
    "EMBEDDING_DIMS = 50 # specify embedding dimesions in ES (we will use PCA -> see below)\n",
    "PRODUCTS_IN_SESSION = [] # list of product ID the user visited in the present session\n",
    "LANGUAGE = 'english'  # put here the ES compatible language string (depending on the language of your catalog/search queries)\n",
    "QUERY1 = 'shoes' # put here the first query to test\n",
    "QUERY2 = 'pants' # put here the second query to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python clients for Redis and ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redis credentials here!\n",
    "REDIS_HOST = 'redis'  # put your redis host here\n",
    "REDIS_PORT = 6379\n",
    "REDIS_DB = 0\n",
    "REDIS_PWD = ''  # password goes here\n",
    "# redis data structure\n",
    "REDIS_HASH_FORMAT = 'product_h'\n",
    "# start redis client\n",
    "redis_client = redis.StrictRedis(host=REDIS_HOST, \n",
    "                                 port=REDIS_PORT, \n",
    "                                 db=REDIS_DB, \n",
    "                                 password=REDIS_PWD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = 'catalog'\n",
    "ES_HOST = {\"host\": \"elasticsearch\", \"port\": 9200}  # change here if you're not using a local ES\n",
    "es_client = Elasticsearch(hosts=[ES_HOST])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_First of all, get products from the catalogue dump into a usable form_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_products_from_catalogue(catalog_file):\n",
    "    \"\"\"\n",
    "    parse catalogue file into a map SKU -> properties (sku, name, target, image url)\n",
    "    \"\"\"\n",
    "    products = {}\n",
    "    with open(catalog_file) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            if row['sku'] and row['image'].endswith('.jpg'):\n",
    "                products[row['sku']] = row\n",
    "    \n",
    "    return products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = get_products_from_catalogue(CATALOGUE_FILE)\n",
    "print('{} products in catalog!'.format(len(products)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, word embeddings, where it all started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embedding_model(training_data):\n",
    "    \"\"\"\n",
    "    training_data is a list of lists (list of words, products, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    # train model with standard params\n",
    "    model = gensim.models.Word2Vec(training_data,\n",
    "                                   min_count=min(len(training_data), 10),\n",
    "                                   size=100,\n",
    "                                   workers=4,\n",
    "                                   window=3,\n",
    "                                   iter=20)\n",
    "    vectors = model.wv\n",
    "    # remove model from memory\n",
    "    del model\n",
    "    \n",
    "    # return vectors as TOKEN -> VECTOR map\n",
    "    return vectors\n",
    "\n",
    "\n",
    "def run_pca(items, dims):\n",
    "    pca = PCA(n_components=dims)\n",
    "    pca_result = pca.fit_transform(items)\n",
    "    exp_variance = pca.explained_variance_ratio_\n",
    "    # print(\"PCA shape {}\".format(pca_result.shape))\n",
    "    # print('Explained variation per component: {}'.format(exp_variance))\n",
    "    print('Cum. exp. var. for {} principal components: {}'.format(dims, np.sum(exp_variance)))\n",
    "\n",
    "    return pca_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_from_corpus(corpus_file, max_sentences=None):\n",
    "    \"\"\"\n",
    "        Read the text file and process it as a list of lists, where each list is \n",
    "        the tokens in a sentence. Don't care too much about pre-processing,\n",
    "        just get stuff done.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    with open(corpus_file) as c_file:\n",
    "        for line in c_file:\n",
    "            # remove punctuation, strip lines, lower case it and normalize spaces\n",
    "            cleaned_line = ' '.join(line.translate(str.maketrans('', '', string.punctuation)).strip().lower().split())\n",
    "            if not cleaned_line:\n",
    "                continue\n",
    "            sentences.append(cleaned_line.split())\n",
    "            # check if we reached a max number of sentences for training\n",
    "            if max_sentences and len(sentences) == max_sentences:\n",
    "                return sentences\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences_data = get_sentences_from_corpus(TEXT_FILE, max_sentences=2000000)\n",
    "print('Total sentences: {}, first is: {}'.format(len(training_sentences_data), training_sentences_data[0]))\n",
    "word_embeddings = train_embedding_model(training_sentences_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Playing with similarities and analogies here_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in ['paris', 'france']:\n",
    "    print('###{}\\n{}\\n'.format(_, word_embeddings.most_similar_cosmul(positive=[_])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_vector_analogy(vectors, man, king, women):\n",
    "    # MAN : KING = WOMAN : ? -> QUEEN\n",
    "    return vectors.most_similar_cosmul(positive=[king, women], negative=[man])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BOY : KING = WOMAN : {}\\n\".format(solve_vector_analogy(word_embeddings, 'boy', 'king', 'girl')[0][0]))\n",
    "print(\"PARIS : FRANCE = BERLIN : {}\\n\".format(solve_vector_analogy(word_embeddings, 'paris', 'france', 'berlin')[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, one more time, with product data this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_products_from_sessions(session_file):\n",
    "    \"\"\"\n",
    "        Our file from the analytics service conveniently dumps, line by line,\n",
    "        user sessions. We just read the file and return a list of lists!\n",
    "        \n",
    "        Every line is:\n",
    "        \n",
    "        LINE_ID (as INT) TAB PRODUCT 1 TAB PRODUCT 2 ...\n",
    "        \n",
    "        P.s.: our file has been processed to include only session with length >= 3 and < 200\n",
    "    \"\"\"\n",
    "    sessions = []\n",
    "    with open(session_file) as session_f:\n",
    "        for line in session_f:\n",
    "            products = line.strip().split('\\t')[1:]\n",
    "            sessions.append(products)\n",
    "        \n",
    "    return sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_session_data = get_products_from_sessions(SESSION_FILE)\n",
    "print('Total sessions: {}, first is: {}'.format(len(training_session_data), training_session_data[0]))\n",
    "product_embeddings = train_embedding_model(training_session_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Check item-item similarity by looking at product vectors close together in the space_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PRODUCT = ''  # fill here with the product ID you want to test\n",
    "\n",
    "matches = product_embeddings.most_similar_cosmul(positive=[TEST_PRODUCT])\n",
    "# display top N\n",
    "for m in matches[:3]:\n",
    "    display(Image(products[m[0]]['image'], width=150, unconfined=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Playing with some analogies here_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill here with your product IDs to test analogies\n",
    "PRODUCT1 = ''\n",
    "PRODUCT1_MATCH = ''\n",
    "PRODUCT2 = ''\n",
    "\n",
    "assert all(_ in product_embeddings.vocab for _ in [PRODUCT1, PRODUCT1_MATCH, PRODUCT2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = solve_vector_analogy(product_embeddings, PRODUCT1, PRODUCT1_MATCH, PRODUCT2)\n",
    "# first show products\n",
    "for _ in [PRODUCT1, PRODUCT1_MATCH, PRODUCT2]:\n",
    "    display(Image(products[_]['image'], width=100, unconfined=True))\n",
    "# then display matches\n",
    "for m in matches[:1]:\n",
    "    if m[0] in products:\n",
    "        display(Image(products[m[0]]['image'], width=100, unconfined=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Finally, we apply pca to reduce vector size and then add them in our product dictionary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_products = [p for p in list(product_embeddings.vocab)]\n",
    "pca_vectors = run_pca([product_embeddings[p] for p in all_products], dims=EMBEDDING_DIMS)\n",
    "sku2vector = {all_products[idx]: list(pca_vectors[idx]) for idx in range(0, len(all_products))}\n",
    "# add vector to products\n",
    "for sku, p in products.items():\n",
    "    p['vector'] = sku2vector.get(p['sku'], None)\n",
    "    p['popularity'] = random.randint(0, 100)  # add a popularity field to fake popularity data for later retrieval\n",
    "# debug\n",
    "print(products[PRODUCT1]['vector'])\n",
    "# remove products without vectors for simplicity\n",
    "products = {k: v for k,v in products.items() if v['vector']}\n",
    "len(products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personalizing search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data into ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_docs_to_es(index_name, docs):\n",
    "    \"\"\"\n",
    "    index_name is a string \n",
    "    docs is a map doc id -> doc as a Python dictionary (in our case SKU -> product)\n",
    "    \"\"\"\n",
    "    # first we delete an index with the same name if any \n",
    "    # ATTENTION: IF YOU USE THIS CODE IN THE REAL WORLD THIS LINE WILL DELETE THE INDEX\n",
    "    if es_client.indices.exists(index_name):\n",
    "        print(\"Deleting {}\".format(index_name))\n",
    "        es_client.indices.delete(index=index_name)    \n",
    "    # next we define our index\n",
    "    body = {\n",
    "        'settings': {\n",
    "            \"number_of_shards\" : 1,\n",
    "            \"number_of_replicas\" : 0\n",
    "        },\n",
    "        \"mappings\": {\n",
    "          \"properties\": {\n",
    "                \"name\": { \"type\": \"text\", \"analyzer\": LANGUAGE },\n",
    "                \"target\": { \"type\": \"text\", \"analyzer\": LANGUAGE },\n",
    "                \"image\": { \"type\": \"text\", \"analyzer\": LANGUAGE },\n",
    "                \"vector\": {\n",
    "                      \"type\": \"dense_vector\",\n",
    "                      \"dims\": EMBEDDING_DIMS\n",
    "                    }\n",
    "                } \n",
    "        }\n",
    "    }\n",
    "    # create index\n",
    "    res = es_client.indices.create(index=index_name, body=body)\n",
    "    # finally, we bulk upload the documents\n",
    "    actions = [{\n",
    "                   \"_index\": index_name,\n",
    "                   \"_id\": sku,\n",
    "                   \"_source\": doc\n",
    "               } for sku, doc in docs.items()\n",
    "            ]\n",
    "    # bulk upload\n",
    "    res = helpers.bulk(es_client, actions)\n",
    "    \n",
    "    return res\n",
    "\n",
    "def query_and_display_results(index_name, search_query, docs, n=5):\n",
    "    res = es_client.search(index=index_name, body=search_query)\n",
    "    print(\"Total hits: {}\\n\".format(res['hits']['total']['value']))\n",
    "    for hit in res['hits']['hits'][:n]:\n",
    "        print('{}\\n'.format(hit[\"_source\"]['sku']))\n",
    "        if hit[\"_source\"]['sku'] in docs:\n",
    "            display(Image(docs[hit[\"_source\"]['sku']]['image'], width=150, unconfined=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_result = upload_docs_to_es(INDEX_NAME, products)\n",
    "upload_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_client.indices.refresh(INDEX_NAME)\n",
    "resp = es_client.get(index=INDEX_NAME, id=PRODUCT1)\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data into our redis cache, to simulate a real-time use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redis_upload(redis_client, rows):\n",
    "    with redis_client.pipeline() as pipe:\n",
    "        for r in rows:\n",
    "            pipe.hset(REDIS_HASH_FORMAT, r['sku'], json.dumps(r))\n",
    "        res = pipe.execute()\n",
    "    \n",
    "    return\n",
    "\n",
    "def load_vectors_to_cache(products, batch_size):\n",
    "    # first we flush the cache\n",
    "    # ATTENTION: IF YOU USE THIS CODE IN THE REAL WORLD THIS LINE WILL DELETE ALL DATA\n",
    "    redis_client.flushall()\n",
    "    # upload data in bulk with pipeline\n",
    "    rows = list(products.values())\n",
    "    for i in range(0, len(rows), batch_size):\n",
    "        print(\"Uploading {} rows {} at {}...\".format(len(rows), i, datetime.utcnow()))\n",
    "        redis_upload(redis_client, rows[i: i + batch_size])\n",
    "    # do some test\n",
    "    print(redis_client.hmget(REDIS_HASH_FORMAT, [r['sku'] for r in rows[:3]]))\n",
    "    #return total number of rows uploaded\n",
    "    return len(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_vectors_to_cache(products, batch_size=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_First, we query ES for a \"vanilla\" search_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = {\n",
    "    \"query\" : {\n",
    "        \"script_score\" : {\n",
    "            \"query\": {\n",
    "                    \"match\" : {\n",
    "                        \"name\" : {\n",
    "                            \"query\" : QUERY1\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "            \"script\": {\n",
    "              \"source\" : \"doc['popularity'].value / 10\"\n",
    "            }\n",
    "        }\n",
    "     }\n",
    "}\n",
    "query_and_display_results(INDEX_NAME, search_query, products, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Now, we retrieve from Redis the vectors for products in the session_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_average_vector(vectors, v_shape):\n",
    "    \"\"\"\n",
    "    not exactly fancy, but... \n",
    "    see for example https://districtdatalabs.silvrback.com/modern-methods-for-sentiment-analysis\n",
    "    \"\"\"\n",
    "    category_vec = np.zeros(v_shape[0]).reshape(v_shape)\n",
    "    count = 0.\n",
    "    for v in vectors:\n",
    "        assert v.shape == category_vec.shape\n",
    "        try:\n",
    "            category_vec += v\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        category_vec/= count\n",
    "        \n",
    "    return category_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_session_vector_from_redis(products_in_session):\n",
    "    session_products = redis_client.hmget(REDIS_HASH_FORMAT, products_in_session)\n",
    "    session_vectors = [np.array(json.loads(s)[\"vector\"]) for s in session_products]\n",
    "    \n",
    "    return build_average_vector(session_vectors, session_vectors[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_vector = retrieve_session_vector_from_redis(PRODUCTS_IN_SESSION)\n",
    "# debug\n",
    "len(session_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(if you don't want to setup Redis, just use the map in memory to retrieve the vectors - uncomment below)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_session_vector_from_memory(products_in_session):\n",
    "    session_vectors = [np.array(products[p]['vector']) for p in products_in_session]\n",
    "    \n",
    "    return build_average_vector(session_vectors, session_vectors[0].shape)\n",
    "\n",
    "# session_vector = retrieve_session_vector_from_memory(products_in_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Finally use the session vector to query_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_query = {\n",
    "    \"query\" : {\n",
    "        \"script_score\" : {\n",
    "            \"query\": {\n",
    "                    \"match\" : {\n",
    "                        \"name\" : {\n",
    "                            \"query\" : QUERY1\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "            \"script\": {\n",
    "              \"source\": \"cosineSimilarity(params.query_vector, doc['vector']) + 1.0\",\n",
    "              \"params\": {\"query_vector\": session_vector.tolist()}\n",
    "            }\n",
    "        }\n",
    "     }\n",
    "}\n",
    "query_and_display_results(INDEX_NAME, vector_query, products, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Try some other query_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanilla query\n",
    "search_query = {\n",
    "    \"query\" : {\n",
    "        \"script_score\" : {\n",
    "            \"query\": {\n",
    "                    \"match\" : {\n",
    "                        \"name\" : {\n",
    "                            \"query\" : QUERY2\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "            \"script\": {\n",
    "              \"source\" : \"doc['popularity'].value / 10\"\n",
    "            }\n",
    "        }\n",
    "     }\n",
    "}\n",
    "query_and_display_results(INDEX_NAME, search_query, products, n=5)\n",
    "# now personalized\n",
    "vector_query = {\n",
    "    \"query\" : {\n",
    "        \"script_score\" : {\n",
    "            \"query\": {\n",
    "                    \"match\" : {\n",
    "                        \"name\" : {\n",
    "                            \"query\" : QUERY2\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "            \"script\": {\n",
    "              \"source\": \"cosineSimilarity(params.query_vector, doc['vector']) + 1.0\",\n",
    "              \"params\": {\"query_vector\": session_vector.tolist()}\n",
    "            }\n",
    "        }\n",
    "     }\n",
    "}\n",
    "query_and_display_results(INDEX_NAME, vector_query, products, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix: how to visualize vectors and impress friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_word_embeddings_tsne(word_embeddings):\n",
    "    # colors\n",
    "    colors = ['red', 'green', 'blue', 'purple', 'yellow', 'black']\n",
    "    interesting_word_groups = [\n",
    "        (['he', 'she', 'it', 'they', 'i', 'you', 'we'], 'pronouns'),\n",
    "        (['london', 'paris', 'berlin', 'budapest', 'amsterdam', 'prague', 'rome'], 'cities'),\n",
    "        (['italy', 'germany', 'spain', 'romania', 'finland', 'poland', 'norway', 'sweden', 'austria', 'brazil'], 'countries'),\n",
    "        (['pasta', 'pizza', 'steak', 'pie', 'fries', 'burger', 'salmon'], 'food'),\n",
    "        (['john', 'mark', 'jane', 'jessica', 'donald', 'simon'], 'names'),\n",
    "        ([random.choice(list(word_embeddings.vocab)) for _ in range(0, 100)], 'other')\n",
    "    ]\n",
    "    all_words = []\n",
    "    for words, group in interesting_word_groups:\n",
    "        for w in words:\n",
    "            all_words.append(w)\n",
    "    all_keys = [w for w in list(word_embeddings.vocab) if w in all_words]\n",
    "    print(len(all_keys))\n",
    "    all_vectors = [word_embeddings[e] for e in all_keys]\n",
    "    # get projection\n",
    "    X_embedded = TSNE(n_components=2).fit_transform(all_vectors)\n",
    "    word_2_emb = {k: e for k, e in zip(all_keys, X_embedded)}\n",
    "    print(len(all_vectors), X_embedded.shape)\n",
    "    # divide groups\n",
    "    data = []\n",
    "    groups = []\n",
    "    for words, group in interesting_word_groups:\n",
    "        groups.append(group)\n",
    "        data.append([word_2_emb[w] for w in words])\n",
    "    print(groups, data[0])\n",
    "    # create plot\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    # add groups\n",
    "    for d, color, group in zip(data, colors, groups):\n",
    "        x = [_[0] for _ in d]\n",
    "        y = [_[1] for _ in d]\n",
    "        ax.scatter(x, y, alpha=0.8, c=color, edgecolors='none', s=30, label=group)\n",
    "    # show plot\n",
    "    plt.title('Plot color-coded embeddings')\n",
    "    plt.legend(loc=2)\n",
    "    plt.show()\n",
    "    \n",
    "    return\n",
    "\n",
    "visualize_word_embeddings_tsne(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_vectors_for_projector_visualization(product_2_vectors,\n",
    "                                               product_2_label,\n",
    "                                               target_folder):\n",
    "    # map dictionary to list to preserve order when exporting\n",
    "    all_p = [p for p in list(product_2_vectors.vocab) if p in product_2_label]\n",
    "    all_v = [product_2_vectors[p] for p in all_p]\n",
    "    # write vectors\n",
    "    with open(os.path.join(target_folder, 'vectors.tsv'), 'w') as v_f:\n",
    "        for idx in range(0, len(all_v)):\n",
    "            v_f.write('{}\\n'.format('\\t'.join(['{:.5f}'.format(_) for _ in pca_res[idx]])))\n",
    "    # if avalaible, labels can be paired with SKUs for visualization purposes\n",
    "    # if a mapping is specified, we produce a \"meta\" file, otherwise we just return\n",
    "    if not product_2_label:\n",
    "        return\n",
    "    # write meta if mapping is available\n",
    "    with open(os.path.join(target_folder, 'meta.tsv', 'w')) as m_f:\n",
    "        # header\n",
    "        m_f.write('sku\\tlabel\\n')\n",
    "        for sku in all_p:\n",
    "            m_f.write('{}\\t{}\\n'.format(sku, product_2_label[sku]))\n",
    "\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
