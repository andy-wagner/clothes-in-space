{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import string\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import redis  # to communicate with redis\n",
    "import gensim # to talk to gensim\n",
    "from IPython.display import Image  # to display URL in noteboook for visual debug\n",
    "from IPython.core.display import display # to display URL in noteboook for visual debug\n",
    "from elasticsearch import Elasticsearch, helpers # remember to !pip install elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Input file variables_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = ''  # local folder here\n",
    "CATALOGUE_FILE = os.path.join(DATA_FOLDER, 'catalog.csv')\n",
    "TEXT_FILE = os.path.join(DATA_FOLDER, 'corpus.txt')  # texts from 1BN words dataset\n",
    "SESSION_FILE = os.path.join(DATA_FOLDER, 'sessions.txt') # file with session data (pre-filtered for length and pre-formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Query variables_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = ''  # put here the ES compatible language string (depending on the language of your catalog/search queries)\n",
    "QUERY1 = '' # put here the first query to test\n",
    "QUERY2 = '' # put here the second query to test\n",
    "TOP_N = 50 # top N results to re-rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Model variables_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIMS = 50 # specify embedding size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Product variables_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRODUCTS_IN_SESSION = [''] # list of product ID the user visited in the present session\n",
    "TEST_PRODUCT = ''  # fill here with the product ID you want to test for similarities\n",
    "# fill here with your product IDs to test for analogies\n",
    "PRODUCT1 = ''\n",
    "PRODUCT1_MATCH = ''\n",
    "PRODUCT2 = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python clients for Redis and ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redis credentials here!\n",
    "REDIS_HOST = 'localhost'\n",
    "REDIS_PORT = 6379\n",
    "REDIS_DB = 0\n",
    "REDIS_PWD = None\n",
    "# redis data structure\n",
    "REDIS_HASH_FORMAT = 'product_h'\n",
    "# start redis client\n",
    "redis_client = redis.StrictRedis(host=REDIS_HOST, \n",
    "                                 port=REDIS_PORT, \n",
    "                                 db=REDIS_DB, \n",
    "                                 password=REDIS_PWD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = 'catalog'\n",
    "ES_HOST = {\"host\": \"localhost\", \"port\": 9200}\n",
    "es_client = Elasticsearch(hosts=[ES_HOST])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_First of all, get products from the catalogue dump into a usable form_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_products_from_catalogue(catalog_file):\n",
    "    \"\"\"\n",
    "    parse catalogue file into a map SKU -> properties (sku, name, target, image url)\n",
    "    \"\"\"\n",
    "    products = {}\n",
    "    with open(catalog_file) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            if row['sku'] and row['image'].endswith('.jpg'):\n",
    "                products[row['sku']] = row\n",
    "    \n",
    "    return products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = get_products_from_catalogue(CATALOGUE_FILE)\n",
    "print('{} products in catalog!'.format(len(products)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, word embeddings, where it all started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embedding_model(training_data):\n",
    "    \"\"\"\n",
    "    training_data is a list of lists (list of words, products, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    # train model with standard params\n",
    "    model = gensim.models.Word2Vec(training_data,\n",
    "                                   min_count=10,\n",
    "                                   size=EMBEDDING_DIMS,\n",
    "                                   workers=4,\n",
    "                                   window=3,\n",
    "                                   iter=20)\n",
    "    vectors = model.wv\n",
    "    # remove model from memory\n",
    "    del model\n",
    "    \n",
    "    # return vectors as TOKEN -> VECTOR map\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_from_corpus(corpus_file, max_sentences=None):\n",
    "    \"\"\"\n",
    "        Read the text file and process it as a list of lists, where each list is \n",
    "        the tokens in a sentence. Don't care too much about pre-processing,\n",
    "        just get stuff done.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    with open(corpus_file) as c_file:\n",
    "        for line in c_file:\n",
    "            # remove punctuation, strip lines, lower case it and normalize spaces\n",
    "            cleaned_line = ' '.join(line.translate(str.maketrans('', '', string.punctuation)).strip().lower().split())\n",
    "            if not cleaned_line:\n",
    "                continue\n",
    "            sentences.append(cleaned_line.split())\n",
    "            # check if we reached a max number of sentences for training\n",
    "            if max_sentences and len(sentences) == max_sentences:\n",
    "                return sentences\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences_data = get_sentences_from_corpus(TEXT_FILE, max_sentences=2000000)\n",
    "print('Total sentences: {}, first is: {}'.format(len(training_sentences_data), training_sentences_data[0]))\n",
    "word_embeddings = train_embedding_model(training_sentences_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Playing with similarities and analogies here_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in ['paris', 'france']:\n",
    "    print('###{}\\n{}\\n'.format(_, word_embeddings.most_similar_cosmul(positive=[_])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_vector_analogy(vectors, man, king, women):\n",
    "    # MAN : KING = WOMAN : ? -> QUEEN\n",
    "    return vectors.most_similar_cosmul(positive=[king, women], negative=[man])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BOY : KING = WOMAN : {}\\n\".format(solve_vector_analogy(word_embeddings, 'boy', 'king', 'girl')[0][0]))\n",
    "print(\"PARIS : FRANCE = BERLIN : {}\\n\".format(solve_vector_analogy(word_embeddings, 'paris', 'france', 'berlin')[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, one more time, with product data this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_products_from_sessions(session_file):\n",
    "    \"\"\"\n",
    "        Our file from the analytics service conveniently dumps, line by line,\n",
    "        user sessions. We just read the file and return a list of lists!\n",
    "        \n",
    "        Every line is:\n",
    "        \n",
    "        LINE_ID (as INT) TAB PRODUCT 1 TAB PRODUCT 2 ...\n",
    "        \n",
    "        P.s.: our file has been pre-processed to include only session with length >= 3 and < 200\n",
    "    \"\"\"\n",
    "    sessions = []\n",
    "    with open(session_file) as session_f:\n",
    "        for line in session_f:\n",
    "            products = line.strip().split('\\t')[1:]\n",
    "            sessions.append(products)\n",
    "        \n",
    "    return sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_session_data = get_products_from_sessions(SESSION_FILE)\n",
    "print('Total sessions: {}, first is: {}'.format(len(training_session_data), training_session_data[0]))\n",
    "product_embeddings = train_embedding_model(training_session_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Check item-item similarity by looking at product vectors close together in the space_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = product_embeddings.most_similar_cosmul(positive=[TEST_PRODUCT])\n",
    "# display top N\n",
    "for m in matches[:3]:\n",
    "    display(Image(products[m[0]]['image'], width=150, unconfined=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Playing with some analogies here_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(_ in product_embeddings.vocab for _ in [PRODUCT1, PRODUCT1_MATCH, PRODUCT2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = solve_vector_analogy(product_embeddings, PRODUCT1, PRODUCT1_MATCH, PRODUCT2)\n",
    "# first show products\n",
    "for _ in [PRODUCT1, PRODUCT1_MATCH, PRODUCT2]:\n",
    "    display(Image(products[_]['image'], width=100, unconfined=True))\n",
    "# then display matches\n",
    "for m in matches[:1]:\n",
    "    if m[0] in products:\n",
    "        display(Image(products[m[0]]['image'], width=100, unconfined=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Finally, we add the vectors to our product dictionary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add vector to products\n",
    "for sku, p in products.items():\n",
    "    p['vector'] = product_embeddings[p['sku']].tolist() if p['sku'] in product_embeddings else None\n",
    "    p['popularity'] = random.randint(0, 100)  # add a popularity field to fake popularity data for later retrieval\n",
    "# remove products without vectors for simplicity\n",
    "products = {k: v for k,v in products.items() if v['vector'] is not None}\n",
    "len(products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personalizing search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_rank_results(session_vector, skus):\n",
    "    results_vectors = retrieve_vectors_from_redis(skus)\n",
    "    distance_matrix = cosine_similarity(session_vector.reshape(1, -1), results_vectors)[0]\n",
    "    so = np.argsort(distance_matrix)\n",
    "    return list(reversed(list(np.array(skus)[so])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data into ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_docs_to_es(index_name, docs):\n",
    "    \"\"\"\n",
    "    index_name is a string \n",
    "    docs is a map doc id -> doc as a Python dictionary (in our case SKU -> product)\n",
    "    \"\"\"\n",
    "    # first we delete an index with the same name if any \n",
    "    # ATTENTION: IF YOU USE THIS CODE IN THE REAL WORLD THIS LINE WILL DELETE THE INDEX\n",
    "    if es_client.indices.exists(index_name):\n",
    "        print(\"Deleting {}\".format(index_name))\n",
    "        es_client.indices.delete(index=index_name)    \n",
    "    # next we define our index\n",
    "    body = {\n",
    "        'settings': {\n",
    "            \"number_of_shards\" : 1,\n",
    "            \"number_of_replicas\" : 0\n",
    "        },\n",
    "        \"mappings\": {\n",
    "          \"properties\": {\n",
    "                \"name\": { \"type\": \"text\", \"analyzer\": LANGUAGE },\n",
    "                \"target\": { \"type\": \"text\", \"analyzer\": LANGUAGE },\n",
    "                \"image\": { \"type\": \"text\", \"analyzer\": LANGUAGE } ,\n",
    "                \"vector\": {\n",
    "                      \"type\": \"dense_vector\",\n",
    "                      \"dims\": EMBEDDING_DIMS\n",
    "                    }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    # create index\n",
    "    res = es_client.indices.create(index=index_name, body=body)\n",
    "    # finally, we bulk upload the documents\n",
    "    actions = [{\n",
    "                   \"_index\": index_name,\n",
    "                   \"_id\": sku,\n",
    "                   \"_source\": doc\n",
    "               } for sku, doc in docs.items()\n",
    "            ]\n",
    "    # bulk upload\n",
    "    res = helpers.bulk(es_client, actions)\n",
    "    \n",
    "    return res\n",
    "\n",
    "def query_with_es(index_name, search_query, n=5):\n",
    "    search_query = {\n",
    "        \"from\": 0,\n",
    "        \"size\": n,\n",
    "        \"query\" : {\n",
    "            \"script_score\" : {\n",
    "                \"query\": {\n",
    "                        \"match\" : {\n",
    "                            \"name\" : {\n",
    "                                \"query\" : search_query\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                \"script\": {\n",
    "                  \"source\" : \"doc['popularity'].value / 10\"\n",
    "                }\n",
    "            }\n",
    "         }\n",
    "    }\n",
    "    res = es_client.search(index=index_name, body=search_query)\n",
    "    print(\"Total hits: {}, returned {}\\n\".format(res['hits']['total']['value'], len(res['hits']['hits'])))\n",
    "    return [(hit[\"_source\"]['sku'], hit[\"_source\"]['image']) for hit in res['hits']['hits']]\n",
    "\n",
    "def query_and_display_results_with_es(index_name, search_query, n=5):\n",
    "    res = query_with_es(index_name, search_query, n=n)\n",
    "    return display_image(res)\n",
    "\n",
    "def display_image(skus, n=5):\n",
    "    for (s, image) in skus[:n]:\n",
    "        print('{} - {}\\n'.format(s, image))\n",
    "        display(Image(image, width=150, unconfined=True))\n",
    "            \n",
    "def query_and_rerank_and_display_results_with_es(index_name, search_query, n, session_vector):\n",
    "    res = query_with_es(index_name, search_query, n=n)\n",
    "    skus = [r[0] for r in res]\n",
    "    re_ranked_sku = re_rank_results(session_vector, skus)\n",
    "\n",
    "    return display_image([(sku, res[skus.index(sku)][1]) for sku in re_ranked_sku])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_result = upload_docs_to_es(INDEX_NAME, products)\n",
    "upload_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_client.indices.refresh(INDEX_NAME)\n",
    "resp = es_client.get(index=INDEX_NAME, id=PRODUCT1)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data into Coveo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_docs_to_coveo(index_name, docs):\n",
    "    # TODO: upload data to coveo\n",
    "    return None\n",
    "\n",
    "def query_with_coveo(index_name, search_query, n=5):\n",
    "    # TODO: query coveo and return a list of tuple (sku, image_url)\n",
    "    return # [(sku1, image1), (sku2, image2), ...]\n",
    "\n",
    "def query_and_display_results_with_coveo(index_name, search_query, n=5):\n",
    "    res = query_with_coveo(index_name, search_query, n=n)\n",
    "    return display_image(res)\n",
    "       \n",
    "def query_and_rerank_and_display_results_with_coveo(index_name, search_query, n, session_vector):\n",
    "    res = query_with_coveo(index_name, search_query, n=n)\n",
    "    return re_rank_results(session_vector, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment here if you want to upload data to coveo as well\n",
    "# upload_result = upload_docs_to_coveo(INDEX_NAME, products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data into Redis, to simulate a real-time use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redis_upload(redis_client, rows):\n",
    "    with redis_client.pipeline() as pipe:\n",
    "        for r in rows:\n",
    "            pipe.hset(REDIS_HASH_FORMAT, r['sku'], json.dumps(r))\n",
    "        res = pipe.execute()\n",
    "    \n",
    "    return\n",
    "\n",
    "def load_vectors_to_cache(products, batch_size):\n",
    "    # first we flush the cache\n",
    "    # ATTENTION: IF YOU USE THIS CODE IN THE REAL WORLD THIS LINE WILL DELETE ALL DATA\n",
    "    redis_client.flushall()\n",
    "    # upload data in bulk with pipeline\n",
    "    rows = list(products.values())\n",
    "    for i in range(0, len(rows), batch_size):\n",
    "        print(\"Uploading {} rows {} at {}...\".format(len(rows), i, datetime.utcnow()))\n",
    "        redis_upload(redis_client, rows[i: i + batch_size])\n",
    "    # do some test\n",
    "    print(redis_client.hmget(REDIS_HASH_FORMAT, [r['sku'] for r in rows[:1]]))\n",
    "    #return total number of rows uploaded\n",
    "    return len(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_vectors_to_cache(products, batch_size=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_First, we query ES for a \"vanilla\" search_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_and_display_results_with_es(INDEX_NAME, QUERY1, TOP_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment here if you like to use Coveo index instead\n",
    "# query_and_display_results_with_coveo(INDEX_NAME, QUERY1, n=TOP_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Now, we retrieve from Redis the vectors for products in the session_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_average_vector(vectors, v_shape):\n",
    "    \"\"\"\n",
    "    not exactly fancy, but... \n",
    "    see for example https://districtdatalabs.silvrback.com/modern-methods-for-sentiment-analysis\n",
    "    \"\"\"\n",
    "    category_vec = np.zeros(v_shape[0]).reshape(v_shape)\n",
    "    for v in vectors:\n",
    "        assert v.shape == category_vec.shape\n",
    "        category_vec += v\n",
    "    \n",
    "    return category_vec / len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_vectors_from_redis(skus):\n",
    "    session_products = redis_client.hmget(REDIS_HASH_FORMAT, skus)\n",
    "    return [np.array(json.loads(s)[\"vector\"]) for s in session_products if s]\n",
    "\n",
    "def retrieve_session_vector_from_redis(products_in_session):\n",
    "    session_vectors = retrieve_vectors_from_redis(products_in_session)\n",
    "    \n",
    "    return build_average_vector(session_vectors, session_vectors[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_vector = retrieve_session_vector_from_redis(PRODUCTS_IN_SESSION)\n",
    "# debug\n",
    "print(len(session_vector), session_vector[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Finally use the session vector to query_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_and_rerank_and_display_results_with_es(INDEX_NAME, QUERY1, TOP_N, session_vector)\n",
    "# uncomment here if you like to use Coveo index instead\n",
    "# query_and_rerank_and_display_results_with_coveo(INDEX_NAME, QUERY1, TOP_N, session_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Try some other query_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanilla query\n",
    "query_and_display_results_with_es(INDEX_NAME, QUERY2, TOP_N)\n",
    "# now personalized\n",
    "query_and_rerank_and_display_results_with_es(INDEX_NAME, QUERY2, TOP_N, session_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix: how to visualize vectors and impress friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_word_embeddings_tsne(word_embeddings):\n",
    "    # colors\n",
    "    colors = ['red', 'green', 'blue', 'purple', 'yellow', 'black']\n",
    "    interesting_word_groups = [\n",
    "        (['he', 'she', 'it', 'they', 'i', 'you', 'we'], 'pronouns'),\n",
    "        (['london', 'paris', 'berlin', 'budapest', 'amsterdam', 'prague', 'rome'], 'cities'),\n",
    "        (['italy', 'germany', 'spain', 'romania', 'finland', 'poland', 'norway', 'sweden', 'austria', 'brazil'], 'countries'),\n",
    "        (['pasta', 'pizza', 'steak', 'pie', 'fries', 'burger', 'salmon'], 'food'),\n",
    "        (['john', 'mark', 'jane', 'jessica', 'donald', 'simon'], 'names'),\n",
    "        ([random.choice(list(word_embeddings.vocab)) for _ in range(0, 100)], 'other')\n",
    "    ]\n",
    "    all_words = []\n",
    "    for words, group in interesting_word_groups:\n",
    "        for w in words:\n",
    "            all_words.append(w)\n",
    "    all_keys = [w for w in list(word_embeddings.vocab) if w in all_words]\n",
    "    all_vectors = [word_embeddings[e] for e in all_keys]\n",
    "    # get projection\n",
    "    X_embedded = TSNE(n_components=2).fit_transform(all_vectors)\n",
    "    word_2_emb = {k: e for k, e in zip(all_keys, X_embedded)}\n",
    "    # divide groups\n",
    "    data = []\n",
    "    groups = []\n",
    "    for words, group in interesting_word_groups:\n",
    "        groups.append(group)\n",
    "        data.append([word_2_emb[w] for w in words])\n",
    "    print(groups, data[0])\n",
    "    # create plot\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    # add groups\n",
    "    for d, color, group in zip(data, colors, groups):\n",
    "        x = [_[0] for _ in d]\n",
    "        y = [_[1] for _ in d]\n",
    "        ax.scatter(x, y, alpha=0.8, c=color, edgecolors='none', s=30, label=group)\n",
    "    # show plot\n",
    "    plt.title('Plot color-coded embeddings')\n",
    "    plt.legend(loc=2)\n",
    "    plt.show()\n",
    "    \n",
    "    return\n",
    "\n",
    "visualize_word_embeddings_tsne(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_vectors_for_projector_visualization(product_2_vectors,\n",
    "                                               product_2_label,\n",
    "                                               target_folder):\n",
    "    # map dictionary to list to preserve order when exporting\n",
    "    all_p = [p for p in list(product_2_vectors.vocab) if (not product_2_label or p in product_2_label)]\n",
    "    all_v = [product_2_vectors[p] for p in all_p]\n",
    "    # write vectors\n",
    "    with open(os.path.join(target_folder, 'vectors.tsv'), 'w') as v_f:\n",
    "        for v in all_v:\n",
    "            v_f.write('{}\\n'.format('\\t'.join(['{:.5f}'.format(_) for _ in v])))\n",
    "    # if avalaible, labels can be paired with SKUs for visualization purposes\n",
    "    # if a mapping is specified, we produce a \"meta\" file, otherwise we just return\n",
    "    if not product_2_label:\n",
    "        return\n",
    "    # write meta if mapping is available\n",
    "    with open(os.path.join(target_folder, 'meta.tsv', 'w')) as m_f:\n",
    "        # header\n",
    "        m_f.write('sku\\tlabel\\n')\n",
    "        for sku in all_p:\n",
    "            m_f.write('{}\\t{}\\n'.format(sku, product_2_label[sku]))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
